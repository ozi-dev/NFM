{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ejiVlGYHe1c",
        "outputId": "3eb8e001-a272-4695-8066-80c623f92c1b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words=nltk.corpus.stopwords.words('english')\n",
        "new_stopwords = ['movie','movies','film','love','good','like','best','great','ring','shawshank','films','one','ive','watch','thing']\n",
        "stop_words.extend(new_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr-0PgdKObmL",
        "outputId": "8e44be9e-1e6d-4f3e-8514-e51232385ea6"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize,sent_tokenize\n",
        "from nltk.stem import PorterStemmer,LancasterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSxTRPJ9NSFo"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/csv_files/reviews.csv', on_bad_lines='skip')\n",
        "df=pd.DataFrame(data)\n",
        "reviews=data[['review.text']]\n",
        "\n",
        "def clean_text(review):\n",
        "  le=WordNetLemmatizer()\n",
        "  word_tokens=word_tokenize(review)\n",
        "  tokens=[le.lemmatize(w) for w in word_tokens if w not in stop_words and len(w)>3]\n",
        "  cleaned_text=\" \".join(tokens)\n",
        "  return cleaned_text\n",
        "\n",
        "df['cleaned_review']=df['review.text'].apply(clean_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_npH35vNs8x",
        "outputId": "83dd0bb2-686a-4ae2-e9c9-3f1ddc82de6d"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(max_features=1500, min_df=10, stop_words=stop_words)\n",
        "X = vectorizer.fit_transform(df['cleaned_review'])\n",
        "words = np.array(vectorizer.get_feature_names())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnbBvxCBNBYw",
        "outputId": "d2a58653-6ef4-4362-a7e2-851a4269658b"
      },
      "outputs": [],
      "source": [
        "nmf = NMF(n_components=10, solver=\"mu\")\n",
        "W = nmf.fit_transform(X)\n",
        "H = nmf.components_\n",
        "\n",
        "for i, topic in enumerate(H):\n",
        "     print(\"Topic {}: {}\".format(i + 1, \",\".join([str(x) for x in words[topic.argsort()[-10:]]])))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
